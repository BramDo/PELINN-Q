{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Liquid Time-Constant (LTC) networks are a continuous-time variant of recurrent neural nets. Each hidden unit is modeled as a dynamical system whose state evolves according to a first-order differential equation rather than a simple discrete update:\n",
        "\n",
        "Hidden state x changes with a rate dx/dt that depends on the current input, the unit’s previous activation, a learned weight matrix θ, and a learnable “conductance” term A.\n",
        "The time constant (the factor that says how quickly the state can change) is also learnable, so the network can adapt to different temporal scales—hence “liquid time-constant”.\n",
        "This makes LTCs more flexible than standard RNNs or LSTMs when dealing with irregular time steps, missing samples, or long gaps in sequences: the dynamics fill in what happens between observations rather than forcing everything onto a fixed clock.\n",
        "\n",
        "In the LTC tutorial notebook:\n",
        "\n",
        "Hidden layer state x₁ is updated by integrating a simple ODE driven by the input I₀ and the previous state x₀, using parameters θ, A, and τ (the time constant).\n",
        "Forward pass computes the updated hidden state and routes it to the output layer as y' = w·x₁ + b.\n",
        "Training uses mean squared error and a straightforward gradient update to fit the toy target function y = 2x + 1.\n",
        "Because the example keeps the “liquid” dynamics extremely simple (scalar parameters and one integration step per sample), it’s a didactic stand-in for the more complex continuous-time solvers used in full LTC models.\n",
        "If you want to see the liquid behavior more clearly, try: (1) letting forward_pass integrate over multiple small time steps, (2) allowing the time constant to change during training, or (3) training on data with uneven sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Lv-LGzoPsP"
      },
      "source": [
        "# Liquid Time-Constant Neural Network (LTC NN) Implementation\n",
        "*Implemented by Michael B.Khani*\n",
        "## Introduction\n",
        "Liquid Time-Constant Neural Networks (LTC NN) are a type of neural network designed to learn complex functions. Their primary characteristic is the unique processing in the hidden layer which relies on prior activations.\n",
        "\n",
        "In this tutorial, we'll implement an LTC NN to approximate the linear function y=2x+1. Our network will comprise of three layers:\n",
        "1.   Input layer\n",
        "2.   Hidden layer (with unique LTC NN processing)\n",
        "3.   Output layer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3STEC2ouopfu"
      },
      "source": [
        "## Step 1: Problem Definition\n",
        "We aim to approximate the function y=2x+1 using a set of input-output pairs as our training data. The goal is to fine-tune the network's parameters such that its predictions are as close as possible to the true outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wppMr6oHo_gx"
      },
      "source": [
        "## Step 2: Data Preparation\n",
        "To approximate our target function, we'll generate training data for the first five integers (0 to 4):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iObpGp9ooHsu"
      },
      "outputs": [],
      "source": [
        "#training_data = [(i, 2*i + 1) for i in range(5)]\n",
        "training_data = [(0, 1), (1, 3), (2, 5), (3, 7), (4, 9)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EwErbwepC3J"
      },
      "source": [
        "## Step 3: Network Architecture\n",
        "1. Input Layer:\n",
        "*   Single node for the input value I_0.\n",
        "2. Hidden Layer:\n",
        "*   Contains two nodes:\n",
        "First node (x_0) always set to 1.0, acting as a bias.\n",
        "Second node (x_1), processing input combined with the prior activation (x_0) using weights in theta and a constant A.\n",
        "3. Output Layer:\n",
        "*   A single node that produces the network's output. It computes the result using activations from the hidden layer combined with weight w and bias b."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPmtM6dhp7Uj"
      },
      "source": [
        "## Step 4: Initialization\n",
        "Before training, we must initialize the network parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tOUsVPG5p8mY"
      },
      "outputs": [],
      "source": [
        "import numpy as np # NumPy is a fundamental package for scientific computing with Python\n",
        "\n",
        "theta = np.array([0.5, -0.2])\n",
        "A = np.array([0.3])\n",
        "tau = 1.0\n",
        "w = 0.4\n",
        "b = -0.1\n",
        "learning_rate = 0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9obs0tDp_9n"
      },
      "source": [
        "## Step 5: Forward Propagation\n",
        "The `forward_pass` function is used to compute the network's output for a given input. It processes data through the layers, applying appropriate weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gXyJrMqpqG2-"
      },
      "outputs": [],
      "source": [
        "def forward_pass(input_value):\n",
        "    I_0 = input_value\n",
        "    x_0 = 1.0  # Use 1.0 instead of 0.0 to avoid vanishing activations initially\n",
        "    S_0 = I_0 * theta[0] * A * x_0 + x_0 * theta[1] * A * x_0\n",
        "    dx_dt_0 = x_0/tau + S_0\n",
        "    x_1 = x_0 + dx_dt_0\n",
        "    y_prime = x_1 * w + b\n",
        "    return y_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example for input_value = 2 using the parameters in LTC_Instructions.ipynb (theta = [0.5, -0.2], A = 0.3, tau = 1.0, w = 0.4, b = -0.1):\n",
        "\n",
        "I_0 = 2, x_0 = 1.0\n",
        "S_0 = I_0*theta[0]*A*x_0 + x_0*theta[1]*A*x_0 = (2*0.5*0.3) + (1*-0.2*0.3) = 0.24\n",
        "dx_dt_0 = x_0/tau + S_0 = 1.0 + 0.24 = 1.24\n",
        "x_1 = x_0 + dx_dt_0 = 1.0 + 1.24 = 2.24\n",
        "y_prime = x_1*w + b = 2.24*0.4 - 0.1 = 0.796\n",
        "If you keep A as the 1-element NumPy array (np.array([0.3])), the code returns the same value wrapped in an array: [0.796]\n",
        "\n",
        "theta = [0.5, -0.2] is just an initialization choice for the two learned coefficients in that toy liquid neuron. The positive 0.5 scales how strongly the current input drives the state, while the negative −0.2 gives a bit of inhibitory/self-damping influence from the existing activation. Picking one positive and one negative value keeps the demonstration interesting (the neuron doesn’t explode or collapse to zero immediately), and the training loop is expected to update them from those starting points. You’re free to pick any other reasonable pair; they’re not derived constants.\n",
        "\n",
        "That second term models self-interaction of the current state. In the simplified liquid neuron recipe in LTC_Instructions.ipynb, S_0 is built from two bilinear parts: one coupling the input with the state (I_0 * theta[0] * A * x_0), and one coupling the state with itself (x_0 * theta[1] * A * x_0). Because the state value x_0 appears in both factors of that second product, it collapses to theta[1] * A * x_0^2. It’s just a compact way to say “scale the recurrent feedback by both the current activation and its own influence,” giving you a quadratic (self-damping or self-excitatory) term controlled by theta[1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBdrAhwqqLja"
      },
      "source": [
        "## Step 6: Loss Calculation\n",
        "We measure how close the network's predictions are to the true outputs using the Mean Squared Error (MSE) loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LyCXisDPqK8-"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(y_prime, desired_output):\n",
        "    return (y_prime - desired_output) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyQ7bbnoqRca"
      },
      "source": [
        "## Step 7: Backward Propagation\n",
        "To optimize the network, we need to adjust its parameters based on the error it makes. The `backward_pass` function computes the gradients required to adjust the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7aMan7iBqVpA"
      },
      "outputs": [],
      "source": [
        "def backward_pass(y_prime, desired_output, input_value, x_t):\n",
        "    dL_dy_prime = 2 * (y_prime - desired_output)\n",
        "    dL_dw = dL_dy_prime * x_t\n",
        "    dL_db = dL_dy_prime\n",
        "    dL_dtheta_0 = dL_dy_prime * w * input_value\n",
        "    dL_dtheta_1 = dL_dy_prime * w * x_t\n",
        "\n",
        "    return dL_dw, dL_db, dL_dtheta_0, dL_dtheta_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hx9CUXjqRAe"
      },
      "source": [
        "## Step 8: Accuracy Evaluation\n",
        "Accuracy quantifies how well our model is performing. We want to know the percentage of predictions that are within a 5% margin of the true outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-6nRdnoMqe1k"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy():\n",
        "    accurate_data_points = 0\n",
        "    for input_value, desired_output in training_data:\n",
        "        y_prime = forward_pass(input_value)\n",
        "        percentage_error = abs((y_prime - desired_output) / desired_output) * 100\n",
        "        if percentage_error < 5:\n",
        "            accurate_data_points += 1\n",
        "    return (accurate_data_points / len(training_data)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IrsHlsmqhaQ"
      },
      "source": [
        "## Step 9: Training the LTC NN\n",
        "Now, we'll iteratively adjust our network's parameters using the training data until our model reaches an accuracy of over 95%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmfVG-kCqnRC",
        "outputId": "62fe7a17-5bae-4496-eb63-ed5ade3087ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss = [12.78858314], Accuracy = 0.0%\n",
            "Loss = [13.75772574], Accuracy = 20.0%\n",
            "Loss = [10.34141747], Accuracy = 20.0%\n",
            "Loss = [6.91020373], Accuracy = 20.0%\n",
            "Loss = [4.72203731], Accuracy = 20.0%\n",
            "Loss = [3.38628261], Accuracy = 20.0%\n",
            "Loss = [2.53838222], Accuracy = 20.0%\n",
            "Loss = [1.97054546], Accuracy = 0.0%\n",
            "Loss = [1.57153825], Accuracy = 0.0%\n",
            "Loss = [1.27979247], Accuracy = 0.0%\n",
            "Loss = [1.05945549], Accuracy = 0.0%\n",
            "Loss = [0.8885917], Accuracy = 0.0%\n",
            "Loss = [0.75317769], Accuracy = 0.0%\n",
            "Loss = [0.64390038], Accuracy = 0.0%\n",
            "Loss = [0.55436818], Accuracy = 0.0%\n",
            "Loss = [0.48006781], Accuracy = 0.0%\n",
            "Loss = [0.41773239], Accuracy = 0.0%\n",
            "Loss = [0.36494519], Accuracy = 0.0%\n",
            "Loss = [0.31988387], Accuracy = 0.0%\n",
            "Loss = [0.28115062], Accuracy = 0.0%\n",
            "Loss = [0.24765696], Accuracy = 0.0%\n",
            "Loss = [0.21854358], Accuracy = 0.0%\n",
            "Loss = [0.19312381], Accuracy = 0.0%\n",
            "Loss = [0.17084275], Accuracy = 20.0%\n",
            "Loss = [0.15124751], Accuracy = 20.0%\n",
            "Loss = [0.133965], Accuracy = 20.0%\n",
            "Loss = [0.11868528], Accuracy = 20.0%\n",
            "Loss = [0.10514887], Accuracy = 20.0%\n",
            "Loss = [0.09313696], Accuracy = 40.0%\n",
            "Loss = [0.08246381], Accuracy = 40.0%\n",
            "Loss = [0.07297073], Accuracy = 40.0%\n",
            "Loss = [0.06452133], Accuracy = 40.0%\n",
            "Loss = [0.05699775], Accuracy = 60.0%\n",
            "Loss = [0.05029759], Accuracy = 60.0%\n",
            "Loss = [0.04433138], Accuracy = 60.0%\n",
            "Loss = [0.03902061], Accuracy = 60.0%\n",
            "Loss = [0.03429607], Accuracy = 60.0%\n",
            "Loss = [0.03009644], Accuracy = 80.0%\n",
            "Loss = [0.02636718], Accuracy = 80.0%\n",
            "Loss = [0.02305961], Accuracy = 80.0%\n",
            "Loss = [0.02013014], Accuracy = 80.0%\n",
            "Loss = [0.01753958], Accuracy = 80.0%\n",
            "Loss = [0.01525264], Accuracy = 80.0%\n",
            "Loss = [0.01323746], Accuracy = 80.0%\n",
            "Loss = [0.01146525], Accuracy = 80.0%\n",
            "Loss = [0.00990994], Accuracy = 80.0%\n",
            "Loss = [0.00854795], Accuracy = 80.0%\n",
            "Loss = [0.00735791], Accuracy = 80.0%\n",
            "Loss = [0.00632051], Accuracy = 100.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4957/838649029.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  theta[0] -= learning_rate * dL_dtheta_0\n",
            "/tmp/ipykernel_4957/838649029.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  theta[1] -= learning_rate * dL_dtheta_1\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "while calculate_accuracy() < 95:\n",
        "    total_loss = 0\n",
        "    for input_value, desired_output in training_data:\n",
        "        y_prime = forward_pass(input_value)\n",
        "        loss = calculate_loss(y_prime, desired_output)\n",
        "        total_loss += loss\n",
        "\n",
        "        dL_dw, dL_db, dL_dtheta_0, dL_dtheta_1 = backward_pass(y_prime, desired_output, input_value, 1.0)  # x_t=1.0\n",
        "\n",
        "        w -= learning_rate * dL_dw\n",
        "        b -= learning_rate * dL_db\n",
        "        theta[0] -= learning_rate * dL_dtheta_0\n",
        "        theta[1] -= learning_rate * dL_dtheta_1\n",
        "\n",
        "    print(f\"Loss = {total_loss}, Accuracy = {calculate_accuracy()}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7fYnw4vqrid"
      },
      "source": [
        "## Step 10: Testing the LTC NN\n",
        "Finally, once the network is trained, we can use it to make predictions and compare them to the actual outputs:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUicQVJzqxBS",
        "outputId": "4f94fe6d-2333-4924-ce6d-365502f33242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Data: [(0, 1), (1, 3), (2, 5), (3, 7), (4, 9)]\n",
            "Input: 0, Predicted Output: [1.04937744], Actual Output: 1\n",
            "Input: 1, Predicted Output: [3.06861444], Actual Output: 3\n",
            "Input: 2, Predicted Output: [5.08785144], Actual Output: 5\n",
            "Input: 3, Predicted Output: [7.10708844], Actual Output: 7\n",
            "Input: 4, Predicted Output: [9.12632544], Actual Output: 9\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "print(f\"\\nTraining Data: {training_data}\")\n",
        "for input_value, _ in training_data:\n",
        "    print(f\"Input: {input_value}, Predicted Output: {forward_pass(input_value)}, Actual Output: {2*(input_value)+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4E1TRb8rIdL"
      },
      "source": [
        "## Conclusion\n",
        "You've now implemented a simple Liquid Time-Constant Neural Network (LTC NN) from scratch and trained it to approximate a linear function. While this example is elementary, the principles applied here can be expanded for more complex tasks.\n",
        "\n",
        "\n",
        "---\n",
        "> Note: Liquid Time-constant Neural networks (LTC NNs) are an interesting architecture that has the advantage of being relatively insensitive to gap length, compared to other RNNs, hidden Markov models, and other sequence learning methods1. While LTC NNs may not be as common as other architectures like CNNs or RNNs, it is always important to consider the nature and requirements of the problem at hand before choosing a suitable neural network architecture.\n",
        "---\n",
        "\n",
        "\n",
        "To get a full understanding, run the provided Python code and analyze the outputs. Experiment by changing parameters, training data, or network structure to gain deeper insights."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "qiskit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
